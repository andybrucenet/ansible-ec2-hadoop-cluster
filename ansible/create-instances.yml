---
###
#
# create-instances.yml
# Written by: Jeffrey Aven
#             Aven Solutions Pty Ltd
#             http://avensolutions.com
#
# Creates:
#   VPC, Subnets, Route Tables, Internet Gateway
#   Security Groups and Specified Hadoop and Edge Node Instances
#
###

- hosts: localhost
  connection: local
  vars_files:
  - "variables.yml"
  gather_facts: False

  tasks:
#
# Create subnet
#   
  - name: Create the VPC subnet
    local_action:
      module: ec2_vpc_subnet
      state: present
      aws_access_key: "{{ access_key }}"
      aws_secret_key: "{{ secret_key }}"
      az: "{{ vpc_subnet.az }}"
      cidr: "{{ vpc_subnet.cidr }}"
      region: "{{ region }}"
      resource_tags: { "Name":"{{ tag_prefix }}-{{ clustername }}-Subnet" }
      validate_certs: no
      vpc_id: "{{ vpc_id }}"
    register: the_vpc_subnet

#
# Create route table
#
  - name: Create the VPC route table
    local_action:
     module: ec2_vpc_route_table
     state: present
     aws_access_key: "{{ access_key }}"
     aws_secret_key: "{{ secret_key }}"
     lookup: tag
     region: "{{ region }}"
     routes: "{{ vpc_route_table.routes }}"
     subnets: "{{ vpc_route_table.subnets }}"
     resource_tags: { "Name":"{{ tag_prefix }}-{{ clustername }}-RouteTable" }
     validate_certs: no
     vpc_id: "{{ vpc_id }}"
    register: the_vpc_route_table

#
# Create Security Groups
#       
  - name: Edge Node - Security Group
    local_action:
     module: ec2_group
     name: "{{ tag_prefix }}-{{ clustername }}-sg-edge"
     description: HDP Edge Node Security Group
     purge_rules: False
     purge_rules_egress: False
     vpc_id: "{{ vpc_id }}"
     region: "{{ region }}"
     aws_access_key: "{{ access_key }}"
     aws_secret_key: "{{ secret_key }}"
     rules:
      - proto: tcp
        from_port: 22
        to_port: 22
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        from_port: 80
        to_port: 80
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        from_port: 8080
        to_port: 8080
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        from_port: 8787
        to_port: 8787
        cidr_ip: 0.0.0.0/0        
      - proto: tcp
        from_port: 7180
        to_port: 7180
        cidr_ip: 0.0.0.0/0   
      - proto: tcp
        from_port: 50000
        to_port: 50100
        cidr_ip: 0.0.0.0/0
# Required for Hue
      - proto: tcp
        from_port: 8000
        to_port: 8000
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        from_port: 8088
        to_port: 8088
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        from_port: 19888
        to_port: 19888
        cidr_ip: 0.0.0.0/0        
# Required for WANDISCO
      - proto: tcp
        from_port: 8082
        to_port: 8083
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        from_port: 7000
        to_port: 7100
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        from_port: 6444
        to_port: 6444
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        from_port: 4321
        to_port: 4321
        cidr_ip: 0.0.0.0/0
      - proto: all
        cidr_ip: "{{ vpc_subnet.cidr }}"
    register: edgenode_sg

#  - debug: var=edgenode_sg

  - name: Edge Node - Tag Security Group
    ec2_tag:
     resource: "{{ edgenode_sg.group_id }}"
     tags: { "Name":"{{ tag_prefix }}-{{ clustername }}-sg-edge" }

  - name: Hadoop Node - Security Group
    local_action:
     module: ec2_group
     name: "{{ tag_prefix }}-{{ clustername }}-sg-hadoop"
     description: HDP Hadoop Security Group
     purge_rules: False
     purge_rules_egress: False
     vpc_id: "{{ vpc_id }}"
     region: "{{ region }}"
     aws_access_key: "{{ access_key }}"
     aws_secret_key: "{{ secret_key }}"
     rules:
      - proto: tcp
        from_port: 22
        to_port: 22
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        from_port: 8042
        to_port: 8042
        cidr_ip: 0.0.0.0/0        
      - proto: all
        group_name: "{{ tag_prefix }}-{{ clustername }}-sg-edge"
      - proto: all
        cidr_ip: "{{ vpc_subnet.cidr }}"
    register: hadoop_sg

  - name: UI Node - Security Group
    local_action:
     module: ec2_group
     name: "{{ tag_prefix }}-{{ clustername }}-sg-ui"
     description: UI Node Security Group
     purge_rules: False
     purge_rules_egress: False
     vpc_id: "{{ vpc_id }}"
     region: "{{ region }}"
     aws_access_key: "{{ access_key }}"
     aws_secret_key: "{{ secret_key }}"
     rules:
      - proto: tcp
        from_port: 22
        to_port: 22
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        from_port: 5901
        to_port: 5901
        cidr_ip: 0.0.0.0/0        
      - proto: all
        group_name: "{{ tag_prefix }}-{{ clustername }}-sg-ui"
      - proto: all
        cidr_ip: "{{ vpc_subnet.cidr }}"
    register: ui_sg

  - name: Hadoop Node - Tag Security Group
    ec2_tag:
     resource: "{{ hadoop_sg.group_id }}"
     tags: { "Name":"{{ tag_prefix }}-{{ clustername }}-sg-hadoop" }

  - name: Master Node - Security Group
    local_action:
     module: ec2_group
     name: "{{ tag_prefix }}-{{ clustername }}-sg-master"
     description: HDP Master Node Security Group
     purge_rules: False
     purge_rules_egress: False
     vpc_id: "{{ vpc_id }}"
     region: "{{ region }}"
     aws_access_key: "{{ access_key }}"
     aws_secret_key: "{{ secret_key }}"
     rules:
      - proto: tcp
        from_port: 50070
        to_port: 50070
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        from_port: 60010
        to_port: 60010
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        from_port: 15000
        to_port: 15000
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        from_port: 9995
        to_port: 9995
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        from_port: 21000
        to_port: 21000
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        from_port: 19888
        to_port: 19888
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        from_port: 8088
        to_port: 8088
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        from_port: 3000
        to_port: 3000
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        from_port: 16010
        to_port: 16010
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        from_port: 11000
        to_port: 11000
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        from_port: 8744
        to_port: 8744
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        from_port: 50095
        to_port: 50095
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        from_port: 8886
        to_port: 8886
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        from_port: 18080
        to_port: 18080
        cidr_ip: 0.0.0.0/0
      - proto: tcp
        from_port: 18081
        to_port: 18081
        cidr_ip: 0.0.0.0/0
      - proto: all
        cidr_ip: "{{ vpc_subnet.cidr }}"
    register: nn_sg    

  - name: Master Node - Tag Security Group
    ec2_tag:
     resource: "{{ nn_sg.group_id }}"
     tags: { "Name":"{{ tag_prefix }}-{{ clustername }}-sg-master" }

  - name: Master Node - Reference Edge Node Security Group
    local_action:
     module: ec2_group
     name: "{{ tag_prefix }}-{{ clustername }}-sg-master"
     description: HDP Edge Node Security Group
     purge_rules: False
     purge_rules_egress: False
     vpc_id: "{{ vpc_id }}"
     region: "{{ region }}"
     aws_access_key: "{{ access_key }}"
     aws_secret_key: "{{ secret_key }}"
     rules:
      - proto: all
        group_name: "{{ tag_prefix }}-{{ clustername }}-sg-edge"

#  - debug: var=the_vpc_subnet

#
# Persistent data setup
#
  - name: Creating local_inventory/ansible-ec2-hadoop-cluster/{{ clustername }} directory if it does not exist 
    file: path=~/.ansible/local_inventory/ansible-ec2-hadoop-cluster/{{ clustername }} state=directory

  - name: Initialize all_nodes file
    shell: truncate -s 0 ~/.ansible/local_inventory/ansible-ec2-hadoop-cluster/{{ clustername }}/all_nodes 

  - name: Initialize hdp.hosts file
    shell: echo -e "127.0.0.1\tlocalhost" > ~/.ansible/local_inventory/ansible-ec2-hadoop-cluster/{{ clustername }}/hdp.hosts 

#
# Create Edge Node        
#
  - name: Create the EC2 Instance for the Edge Node
    local_action:
     module: ec2
     aws_access_key: "{{ access_key }}"
     aws_secret_key: "{{ secret_key }}"
     region: "{{ region }}"
     key_name: "{{ pemkey_name }}"
     group_id: "{{ edgenode_sg.group_id }}"
     instance_type: "{{ edgenode_instancetype }}"
     image: "{{ imageid }}"
     vpc_subnet_id: "{{ the_vpc_subnet.subnet.id }}"
     assign_public_ip: yes
     wait: yes
     monitoring: yes
     volumes:
      - device_name: /dev/sda1
        volume_size: "{{ root_ebs_size }}"
        delete_on_termination: True
     instance_tags:
      Name: "{{ tag_prefix }}-{{ clustername }}-edge"
      Cluster: "{{ tag_prefix }}-{{ clustername }}"
    register: ec2_edgenode

  - name: Add edge node to inventory
    shell: add-host.sh --path ~/.ansible/local_inventory/ansible-ec2-hadoop-cluster/{{ clustername }}/all_nodes --hostname-public={{ ec2_edgenode.instances[0].public_dns_name }} --hostname-internal=edge --groups edge allnodes --opt-arg-keys ansible_ssh_private_key_file ansible_ssh_user --opt-arg-values {{ pemkey }} {{ ssh_login }}

  - name: Add edge node to hdp.hosts file
    shell: echo -e "{{ ec2_edgenode.instances[0].private_ip }}\tedge.{{ domainname }}" >> ~/.ansible/local_inventory/ansible-ec2-hadoop-cluster/{{ clustername }}/hdp.hosts

#
# Create Master Nodes
#    
  - name: Create the EC2 Instances for the Master Nodes
    local_action:
     module: ec2
     aws_access_key: "{{ access_key }}"
     aws_secret_key: "{{ secret_key }}"
     region: "{{ region }}"
     key_name: "{{ pemkey_name }}"
     group_id: ["{{ hadoop_sg.group_id }}","{{ nn_sg.group_id }}"]
     instance_type: "{{ hdpmaster_instancetype }}"
     image: "{{ imageid }}"
     vpc_subnet_id: "{{ the_vpc_subnet.subnet.id }}"
     assign_public_ip: yes
     wait: yes
     monitoring: yes
     volumes:
      - device_name: /dev/sda1
        volume_size: "{{ root_ebs_size }}"
        delete_on_termination: True
     instance_tags:
      Name: "{{ tag_prefix }}-{{ clustername }}-master{{ item }}"
      Cluster: "{{ tag_prefix }}-{{ clustername }}"
    register: ec2_masters
    with_sequence: count=2

  - name: Add master nodes to inventory
    shell: add-host.sh --path ~/.ansible/local_inventory/ansible-ec2-hadoop-cluster/{{ clustername }}/all_nodes --hostname-public={{ item.1.instances[0].public_dns_name }} --hostname-internal=master{{ item.0 + 1 }} --groups master master{{ item.0 + 1 }} allnodes --opt-arg-keys ansible_ssh_private_key_file ansible_ssh_user --opt-arg-values {{ pemkey }} {{ ssh_login }}
    with_indexed_items: "{{ ec2_masters.results }}"

  - name: Add master nodes to hdp.hosts file
    shell: echo -e "{{ item.1.instances[0].private_ip }}\tmaster{{ item.0 + 1 }}.{{ domainname }}" >> ~/.ansible/local_inventory/ansible-ec2-hadoop-cluster/{{ clustername }}/hdp.hosts
    with_indexed_items: "{{ ec2_masters.results }}"    

#
# Create Tools Node
#    
  - name: Create the EC2 Instance for the Tools Nodes
    local_action:
     module: ec2
     aws_access_key: "{{ access_key }}"
     aws_secret_key: "{{ secret_key }}"
     region: "{{ region }}"
     key_name: "{{ pemkey_name }}"
     group_id: ["{{ hadoop_sg.group_id }}","{{ nn_sg.group_id }}"]
     instance_type: "{{ hdptools_instancetype }}"
     image: "{{ imageid }}"
     vpc_subnet_id: "{{ the_vpc_subnet.subnet.id }}"
     assign_public_ip: yes
     wait: yes
     monitoring: yes
     volumes:
      - device_name: /dev/sda1
        volume_size: "{{ root_ebs_size }}"
        delete_on_termination: True
     instance_tags:
      Name: "{{ tag_prefix }}-{{ clustername }}-tools{{ item }}"
      Cluster: "{{ tag_prefix }}-{{ clustername }}"
    register: ec2_tools
    with_sequence: count=1

  - name: Add tools nodes to inventory
    shell: add-host.sh --path ~/.ansible/local_inventory/ansible-ec2-hadoop-cluster/{{ clustername }}/all_nodes --hostname-public={{ item.1.instances[0].public_dns_name }} --hostname-internal=tools{{ item.0 + 1 }} --groups tools tools{{ item.0 + 1 }} allnodes --opt-arg-keys ansible_ssh_private_key_file ansible_ssh_user --opt-arg-values {{ pemkey }} {{ ssh_login }}
    with_indexed_items: "{{ ec2_tools.results }}"

  - name: Add tools nodes to hdp.hosts file
    shell: echo -e "{{ item.1.instances[0].private_ip }}\ttools{{ item.0 + 1 }}.{{ domainname }}" >> ~/.ansible/local_inventory/ansible-ec2-hadoop-cluster/{{ clustername }}/hdp.hosts
    with_indexed_items: "{{ ec2_tools.results }}"    

#
# Create Data Nodes
#       
  - name: Create the EC2 Instances for the Data Nodes
    local_action:
     module: ec2
     aws_access_key: "{{ access_key }}"
     aws_secret_key: "{{ secret_key }}"
     region: "{{ region }}"
     key_name: "{{ pemkey_name }}"
     group_id: "{{ hadoop_sg.group_id }}"
     instance_type: "{{ hdpdata_instancetype }}"
     image: "{{ imageid }}"
     vpc_subnet_id: "{{ the_vpc_subnet.subnet.id }}"
     assign_public_ip: yes
     wait: yes
     monitoring: yes
     volumes:
      - device_name: /dev/sda1
        volume_size: "{{ root_ebs_size }}"
        delete_on_termination: True
     instance_tags:
      Name: "{{ tag_prefix }}-{{ clustername }}-data{{ item }}"
      Cluster: "{{ tag_prefix }}-{{ clustername }}"
    register: ec2_datas
    with_sequence: count={{ number_of_nodes }}

  - name: Add data nodes to inventory
    shell: add-host.sh --path ~/.ansible/local_inventory/ansible-ec2-hadoop-cluster/{{ clustername }}/all_nodes --hostname-public={{ item.1.instances[0].public_dns_name }} --hostname-internal=data{{ item.0 + 1 }} --groups data{{ item.0 + 1 }} data allnodes --opt-arg-keys ansible_ssh_private_key_file ansible_ssh_user --opt-arg-values {{ pemkey }} {{ ssh_login }}
    with_indexed_items: "{{ ec2_datas.results }}"

  - name: Add data nodes to hdp.hosts file
    shell: echo -e "{{ item.1.instances[0].private_ip }}\tdata{{ item.0 + 1 }}.{{ domainname }}" >> ~/.ansible/local_inventory/ansible-ec2-hadoop-cluster/{{ clustername }}/hdp.hosts
    with_indexed_items: "{{ ec2_datas.results }}"    

#
# Create "Extra" Data Nodes
#       
  - name: Create the EC2 Instances for the Extra Data Nodes
    local_action:
     module: ec2
     aws_access_key: "{{ access_key }}"
     aws_secret_key: "{{ secret_key }}"
     region: "{{ region }}"
     key_name: "{{ pemkey_name }}"
     group_id: "{{ hadoop_sg.group_id }}"
     instance_type: "{{ hdpdata_instancetype }}"
     image: "{{ imageid }}"
     vpc_subnet_id: "{{ the_vpc_subnet.subnet.id }}"
     assign_public_ip: yes
     wait: yes
     monitoring: yes
     volumes:
      - device_name: /dev/sda1
        volume_size: "{{ root_ebs_size }}"
        delete_on_termination: True
     instance_tags:
      Name: "{{ tag_prefix }}-{{ clustername }}-extra{{ item }}"
      Cluster: "{{ tag_prefix }}-{{ clustername }}"
    register: ec2_extras
    with_sequence: count={{ number_of_extras }}

  - name: Add extra nodes to inventory
    shell: add-host.sh --path ~/.ansible/local_inventory/ansible-ec2-hadoop-cluster/{{ clustername }}/all_nodes --hostname-public={{ item.1.instances[0].public_dns_name }} --hostname-internal=extra{{ item.0 + 1 }} --groups extra allnodes --opt-arg-keys ansible_ssh_private_key_file ansible_ssh_user --opt-arg-values {{ pemkey }} {{ ssh_login }}
    with_indexed_items: "{{ ec2_extras.results }}"

  - name: Add extra nodes to hdp.hosts file
    shell: echo -e "{{ item.1.instances[0].private_ip }}\textra{{ item.0 + 1 }}.{{ domainname }}" >> ~/.ansible/local_inventory/ansible-ec2-hadoop-cluster/{{ clustername }}/hdp.hosts
    with_indexed_items: "{{ ec2_extras.results }}"    

#
# Create "UI" Nodes
#       
  - name: Create the EC2 Instances for the UI node
    local_action:
     module: ec2
     aws_access_key: "{{ access_key }}"
     aws_secret_key: "{{ secret_key }}"
     region: "{{ region }}"
     key_name: "{{ pemkey_name }}"
     group_id: "{{ ui_sg.group_id }}"
     instance_type: "m3.xlarge"
     image: "{{ imageid }}"
     vpc_subnet_id: "{{ the_vpc_subnet.subnet.id }}"
     assign_public_ip: yes  
     wait: yes
     monitoring: yes
     volumes:
      - device_name: /dev/sda1
        volume_size: "20"
        delete_on_termination: True
     instance_tags:
      Name: "{{ tag_prefix }}-{{ clustername }}-ui{{ item }}"
      Cluster: "{{ tag_prefix }}-{{ clustername }}"
    register: ec2_uis
    with_sequence: count=1

  - name: Add UI nodes to inventory
    shell: add-host.sh --path ~/.ansible/local_inventory/ansible-ec2-hadoop-cluster/{{ clustername }}/all_nodes --hostname-public={{ item.1.instances[0].public_dns_name }} --hostname-internal=ui{{ item.0 + 1 }} --groups ui allnodes --opt-arg-keys ansible_ssh_private_key_file ansible_ssh_user --opt-arg-values {{ pemkey }} {{ ssh_login }}
    with_indexed_items: "{{ ec2_uis.results }}"

  - name: Add UI nodes to hdp.hosts file
    shell: echo -e "{{ item.1.instances[0].private_ip }}\tui{{ item.0 + 1 }}.{{ domainname }}" >> ~/.ansible/local_inventory/ansible-ec2-hadoop-cluster/{{ clustername }}/hdp.hosts
    with_indexed_items: "{{ ec2_uis.results }}"    

